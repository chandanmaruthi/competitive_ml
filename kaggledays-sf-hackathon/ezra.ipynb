{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./data/train_2.csv\", na_values=['NA','?', '-1'])\n",
    "test_df = pd.read_csv(\"data/test_2.csv\", na_values=['NA','?', '-1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399335, 201)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape\n",
    "test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          int64\n",
       "target      int64\n",
       "A_1       float64\n",
       "A_2       float64\n",
       "B_1       float64\n",
       "B_2       float64\n",
       "B_3       float64\n",
       "B_4       float64\n",
       "B_5        object\n",
       "B_6        object\n",
       "B_7       float64\n",
       "B_8       float64\n",
       "B_9       float64\n",
       "B_10      float64\n",
       "B_11      float64\n",
       "B_12        int64\n",
       "B_13      float64\n",
       "B_14       object\n",
       "B_15       object\n",
       "C_1       float64\n",
       "C_2       float64\n",
       "C_3       float64\n",
       "C_4       float64\n",
       "C_5       float64\n",
       "C_6       float64\n",
       "C_7       float64\n",
       "C_8       float64\n",
       "C_9       float64\n",
       "C_10      float64\n",
       "C_11      float64\n",
       "           ...   \n",
       "D_137     float64\n",
       "D_138     float64\n",
       "D_139     float64\n",
       "D_140     float64\n",
       "D_141     float64\n",
       "D_142     float64\n",
       "D_143     float64\n",
       "D_144     float64\n",
       "D_145     float64\n",
       "D_146     float64\n",
       "D_147     float64\n",
       "D_148     float64\n",
       "D_149     float64\n",
       "D_150     float64\n",
       "D_151     float64\n",
       "D_152     float64\n",
       "D_153     float64\n",
       "D_154     float64\n",
       "D_155     float64\n",
       "D_156     float64\n",
       "D_157     float64\n",
       "D_158     float64\n",
       "D_159     float64\n",
       "D_160     float64\n",
       "D_161     float64\n",
       "D_162     float64\n",
       "D_163     float64\n",
       "D_164     float64\n",
       "D_165     float64\n",
       "D_166     float64\n",
       "Length: 202, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'B' 'C' 'D' 'A' 'F' 'E']\n",
      "[nan 'B' 'A' 'C']\n",
      "['A' nan 'B']\n",
      "[nan 'Q' 'b' 'j' 'D' 'C' 'c' 'f' 'T' 'N' 'F' 'Y' 'J' 'g' 'A' 'U' 'S' 'l'\n",
      " 'm' 'n' 'K' 'O' 'I' 'X' 'B' 'H' 'a' 'p' 'R' 'L' 'V' 'd' 'E' 'P' 'G' 'e'\n",
      " 'W' 'M' 'i' 'o' 'h' 'k']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.B_5.unique())\n",
    "print(train_df.B_6.unique())\n",
    "print(train_df.B_14.unique())\n",
    "print(train_df.B_15.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "minmax = pd.DataFrame()\n",
    "standard = pd.DataFrame()\n",
    "cont = list()\n",
    "cont = [\"B_3\", \"B_4\", \"B_8\", \"B_10\", \"B_11\", \"B_12\", \"C_1\", \"C_2\", \"C_3\", \"C_4\", \"C_5\", \"C_6\", \"C_7\", \"C_8\", \"C_9\", \"C_10\", \"C_11\", \"C_12\", \"C_13\", \"C_14\", \"C_15\", \"C_16\", \"C_17\"] \n",
    "for i in range(166):\n",
    "    col = \"D_\" + str(i+1)\n",
    "    cont.append(col)\n",
    "for category in cont:\n",
    "    for row in train_df[train_df[category].isnull()].itertuples():\n",
    "      train_df.loc[row[0],category] = train_df[category].mean()\n",
    "    minmax[category+\"_minmax\"] = preprocessing.minmax_scale(train_df[category])\n",
    "    standard[category+\"_standard\"] = preprocessing.scale(train_df[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "encoder = ce.TargetEncoder(cols=[\"B_5\", \"B_6\", \"B_14\", \"B_15\"], return_df=True)\n",
    "X = train_df\n",
    "y = train_df['target']\n",
    "train2_df = encoder.fit_transform(X,y)\n",
    "train3_df = pd.get_dummies(train_df, dummy_na=True, columns=[\"B_5\", \"B_6\", \"B_14\", \"B_15\"])\n",
    "train_df = train_df.drop([\"id\", \"B_5\", \"B_6\", \"B_14\", \"B_15\"], axis=1)\n",
    "train2_df = train2_df.drop([\"id\", \"target\", \"A_1\", \"A_2\", \"B_1\", \"B_2\", \"B_3\", \"B_4\", \"B_7\", \"B_8\", \"B_9\", \"B_10\", \"B_11\", \"B_12\", \"B_13\", \"C_1\", \"C_2\", \"C_3\", \"C_4\", \"C_5\", \"C_6\", \"C_7\", \"C_8\", \"C_9\", \"C_10\", \"C_11\", \"C_12\", \"C_13\", \"C_14\", \"C_15\", \"C_16\", \"C_17\"], axis=1)\n",
    "train3_df = train3_df.drop([\"id\", \"target\", \"A_1\", \"A_2\", \"B_1\", \"B_2\", \"B_3\", \"B_4\", \"B_7\", \"B_8\", \"B_9\", \"B_10\", \"B_11\", \"B_12\", \"B_13\", \"C_1\", \"C_2\", \"C_3\", \"C_4\", \"C_5\", \"C_6\", \"C_7\", \"C_8\", \"C_9\", \"C_10\", \"C_11\", \"C_12\", \"C_13\", \"C_14\", \"C_15\", \"C_16\", \"C_17\"], axis=1)\n",
    "train_df = pd.concat([train_df, train2_df], axis=1)\n",
    "train_df = pd.concat([train_df, train3_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, minmax], axis=1)\n",
    "train_df = pd.concat([train_df, standard], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400665, 137)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names must be unique",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-9832f864b34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m        \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m194\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m        silent=False, subsample=1)\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 706\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.n_estimators,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mfeature_names\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_names must be unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'feature_names must have the same length as data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names must be unique"
     ]
    }
   ],
   "source": [
    "\n",
    "# First XGBoost model for Pima Indians dataset\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "# load data\n",
    "dataset = train_df\n",
    "\n",
    "# split data into X and y\n",
    "X = dataset.drop(['target'], axis=1)\n",
    "Y = dataset['target']\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=6, min_child_weight=1, missing=None, n_estimators=1000,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=194, seed=None,\n",
    "       silent=False, subsample=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d3/82a4b85a87ece114f6d0139d643580c726efa45fa4db3b81aed38c0156c5/category_encoders-1.3.0-py2.py3-none-any.whl\n",
      "Collecting statsmodels>=0.6.1 (from category_encoders)\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/75/758f980df4d971b909d2bc516d22494a2e871fe1f3968ff9798b52d20fb9/statsmodels-0.9.0-cp35-cp35m-manylinux1_x86_64.whl (7.3MB)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.5/site-packages (from category_encoders)\n",
      "Requirement already satisfied: pandas>=0.20.1 in /usr/local/lib/python3.5/dist-packages (from category_encoders)\n",
      "Collecting patsy>=0.4.1 (from category_encoders)\n",
      "  Using cached https://files.pythonhosted.org/packages/ea/0c/5f61f1a3d4385d6bf83b83ea495068857ff8dfb89e74824c6e9eb63286d8/patsy-0.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.5/site-packages (from category_encoders)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/site-packages (from category_encoders)\n",
      "Requirement already satisfied: mkl-random in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->category_encoders)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->category_encoders)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->category_encoders)\n",
      "Requirement already satisfied: icc-rt in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->category_encoders)\n",
      "Requirement already satisfied: mkl-fft in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->category_encoders)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas>=0.20.1->category_encoders)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas>=0.20.1->category_encoders)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from patsy>=0.4.1->category_encoders)\n",
      "Requirement already satisfied: intel-scipy in /usr/local/lib/python3.5/dist-packages (from scikit-learn>=0.17.1->category_encoders)\n",
      "Requirement already satisfied: pydaal in /usr/local/lib/python3.5/dist-packages (from scikit-learn>=0.17.1->category_encoders)\n",
      "Requirement already satisfied: intel-numpy in /usr/local/lib/python3.5/dist-packages (from scipy>=0.17.0->category_encoders)\n",
      "Requirement already satisfied: tbb==2019.* in /usr/local/lib/python3.5/dist-packages (from tbb4py->numpy>=1.11.1->category_encoders)\n",
      "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.5/dist-packages (from mkl->numpy>=1.11.1->category_encoders)\n",
      "Requirement already satisfied: daal==2019.* in /usr/local/lib/python3.5/dist-packages (from pydaal->scikit-learn>=0.17.1->category_encoders)\n",
      "Installing collected packages: patsy, statsmodels, category-encoders\n",
      "Successfully installed category-encoders-1.3.0 patsy-0.5.1 statsmodels-0.9.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip3 install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

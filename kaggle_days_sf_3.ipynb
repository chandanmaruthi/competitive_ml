{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from warnings import filterwarnings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn import svm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, KBinsDiscretizer\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "do_sample = False\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "if do_sample:\n",
    "    train = train.sample(frac=0.1, random_state=1)\n",
    "    \n",
    "\n",
    "print(\"train data shape\", train.shape)\n",
    "print(\"test data shape\", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"A_avg\"] = (train[\"A_1\"]+train[\"A_2\"]) / 2\n",
    "# for i in range(165):\n",
    "#     train[\"XD_\"+str(i+1)+str(i+2)] = (train[\"D_\"+str(i+1)]+train[\"D_\"+str(i+2)])/2 \n",
    "    \n",
    "# train.fillna(value=-1)\n",
    "# test.fillna(value=-1)\n",
    "\n",
    "# target_column = \"target\"\n",
    "# id_column = \"id\"\n",
    "# categorical_cols = [c for c in test.columns if test[c].dtype in [np.object]]\n",
    "# numerical_cols = [c for c in test.columns if test[c].dtype in [np.float, np.int] and c not in [target_column, id_column]]\n",
    "# print(\"Number of features\", len(categorical_cols)+len(numerical_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train.drop(['id','target','B_15'],axis = 1)\n",
    "test_input  = test.drop(['id','B_15'],axis = 1)\n",
    "\n",
    "train_labels = train['target']\n",
    "\n",
    "app_train = pd.get_dummies(train_input)\n",
    "app_test = pd.get_dummies(test_input)\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(app_train)\n",
    "train_imputed = imp_mean.transform(app_train)\n",
    "test_imputed = imp_mean.transform(app_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_imputed)\n",
    "train_imputed = scaler.transform(train_imputed)\n",
    "test_imputed = scaler.transform(test_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = list(app_train.columns)\n",
    "random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n",
    "random_forest.fit(train_imputed,train_labels)\n",
    "feature_importance_values = random_forest.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': features, 'importance':feature_importance_values})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    #Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    #Normalise the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    \n",
    "    #Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10,6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    #Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))),\n",
    "           df['importance_normalized'].head(15),\n",
    "           align = 'center', edgecolor = 'k')\n",
    "    #Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    #Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_threshold = 0.001\n",
    "#print(feature_importances_sorted.query('importance_normalized>{}'.format(fe_threshold)))\n",
    "selected_features = feature_importances_sorted.query('importance_normalized > {}'.format(fe_threshold))['feature'].values\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed= pd.DataFrame(train_imputed, columns = app_train.columns)\n",
    "test_imputed= pd.DataFrame(test_imputed, columns= app_test.columns)\n",
    "train_imputed=pd.concat([train_imputed[selected_features], train_labels])\n",
    "test_imputed=test_imputed[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column ='target'\n",
    "id_column = 'id'\n",
    "categorical_cols = [c for c in test.columns if test[c].dtype in [np.object]]\n",
    "numerical_cols = [c for c in test.columns if test[c].dtype in [np.float, np.int] and c not in [target_column, id_column]]\n",
    "preprocess = make_column_transformer(\n",
    "    (numerical_cols, make_pipeline(SimpleImputer(), StandardScaler())),\n",
    "    (categorical_cols, OneHotEncoder()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lgbm = make_pipeline(preprocess,LGBMClassifier(n_jobs=-1,eta=0.01,max_depth=5,max_bin=512,learning_rate=0.01,num_iterations=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_xgb = make_pipeline(\n",
    "    preprocess,\n",
    "    XGBClassifier(n_jobs=-1, nthreads=-1)   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lr = make_pipeline(\n",
    "    preprocess,\n",
    "    LogisticRegression(n_jobs=-1)   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gnb = make_pipeline(\n",
    "    preprocess,\n",
    "    GaussianNB()   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rf = make_pipeline(preprocess,RandomForestClassifier(random_state=42))\n",
    "classifier_svm = make_pipeline(preprocess, svm.SVC(random_state=42))\n",
    "#pipe_lr_pca = make_pipeline(column_transformer, LogisticRegression(random_state=42))\n",
    "#pipe_rf_pca = make_pipeline(column_transformer,RandomForestClassifier(random_state=42))\n",
    "#pipe_svm_pca = make_pipeline( svm.SVC(column_transformer,random_state=42))\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('lgbm', classifier_lgbm), \n",
    "    ('xgb', classifier_xgb), \n",
    "    ('rf', classifier_rf),\n",
    "    ('lr',classifier_lr), \n",
    "    ('gnb', classifier_gnb),\n",
    "    ('classifier_svm',classifier_svm )],\n",
    "                 voting='soft', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "oof_pred = cross_val_predict(classifier_lgbm, \n",
    "                             train, \n",
    "                             train[target_column], \n",
    "                             cv=5,\n",
    "                             method=\"predict_proba\",\n",
    "                                  n_jobs=-1,\n",
    "                                  verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross validation AUC {:.4f}\".format(roc_auc_score(train[target_column], oof_pred[:,1])))\n",
    "classifier_lgbm.fit(train, train[target_column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = classifier_lgbm.predict_proba(test)[:,1]\n",
    "sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "sub[target_column] = test_preds\n",
    "sub.to_csv(\"results/submit_results.csv\", index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kaggle competitions submit -c kaggledays-sf-hackathon -f ./results/submit_results.csv -m \"crawl\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
